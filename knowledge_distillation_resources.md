<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> •
  <a href="https://github.com/nebuly-ai/learning-AI-optimization#contribute">Contribute to the library</a>
</p>


<img height="25" width="100%" src="https://user-images.githubusercontent.com/83510798/171454644-d4b980bc-15ab-4a31-847c-75c36c5bd96b.png">


# Resources

Awesome resource collection on quantization techniques.

- <a href="#literature-reviews">Literature reviews</a> and <a href="#papers">papers</a>
- <a href="#courses-webinars-and-blogs">Courses, webinars and blogs</a>

If you still didn't, check [Overview page on quantization](https://github.com/nebuly-ai/learning-AI-optimization/blob/main/quantization-resources) or [Exploring AI optimization](https://github.com/nebuly-ai/exploring-AI-optimization) to start getting into it. 



## Literature reviews and papers
Legend: 
- ✏️  100-499 citations, ✏️✏️ $\geq$ 500 citations
- ⭐  100-249 stars, ⭐⭐ $\geq$ 250 stars

Sorting: chronological/alphabetic order
<br> 

### Literature reviews

-Title + link


### Papers
2022
- 8-bit Optimizers via Block-wise Quantization [[ICLR](https://arxiv.org/abs/2110.02861)]


2021
- xxx

2020
- xxxxxx


## Courses, webinars and blogs

Webinars, video content
- Title + link

Blogs, written content
- Title + link



<img height="25" width="100%" src="https://user-images.githubusercontent.com/83510798/171454644-d4b980bc-15ab-4a31-847c-75c36c5bd96b.png">

<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> •
  <a href="https://github.com/nebuly-ai/learning-AI-optimization#contribute">Contribute to the library</a>
</p>
