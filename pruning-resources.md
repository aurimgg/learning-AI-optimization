
<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> •
  <a href="https://github.com/nebuly-ai/learning-AI-optimization#contribute">Contribute to the library</a>
</p>


<img height="25" width="100%" src="https://user-images.githubusercontent.com/83510798/171454644-d4b980bc-15ab-4a31-847c-75c36c5bd96b.png">


# Resources

Awesome resource collection on pruning techniques:

- <a href="#literature-reviews">Literature reviews</a> and <a href="#papers">papers</a>
- <a href="#courses-webinars-and-blogs">Courses, webinars and blogs</a>

If you are new to pruning check the [overview page on pruning](https://github.com/nebuly-ai/exploring-AI-optimization/blob/main/pruning-overview.md) or the introduction to AI optimization at this [link](https://github.com/nebuly-ai/exploring-AI-optimization). 


## Literature reviews and papers
Legenda: 
- ✏️  100-499 citations, ✏️✏️ $\geq$ 500 citations
- ⭐  100-249 stars, ⭐⭐ $\geq$ 250 stars

Sorting: typology / chronological / alphabetical order
<br>

### Literature reviews
- A Survey of Model Compression and Acceleration for Deep Neural Networks [[✏️✏️paper](https://arxiv.org/abs/1710.09282)]
- APQ: Joint Search for Network Architecture, Pruning and Quantization Policy [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_APQ_Joint_Search_for_Network_Architecture_Pruning_and_Quantization_Policy_CVPR_2020_paper.html)][⭐⭐[github](https://github.com/mit-han-lab/apq)]
- Compression of Deep Learning Models for Text: A Survey [[✏️paper](https://arxiv.org/pdf/2008.05221.pdf)]
- Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask [[✏️NIPS](https://proceedings.neurips.cc/paper/2019/hash/1113d7a76ffceca1bb350bfe145467c6-Abstract.html)]
- Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding [[✏️✏️paper](https://arxiv.org/abs/1510.00149)]
- Dynamic Network Surgery for Efficient DNNs [[✏️✏️NIPS](https://proceedings.neurips.cc/paper/2016/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html)]
- Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better Models Smaller, Faster, and Better [[paper](https://arxiv.org/pdf/2106.08962.pdf)][⭐⭐[github](https://github.com/reddragon/efficient-dl-survey-paper)]
- ETH 2021, Sparsity in Deep Learning: Pruning + growth for efficient inference and training in neural networks: [[✏️JLMR](https://www.jmlr.org/papers/volume22/21-0366/21-0366.pdf)][⭐⭐[github](https://github.com/spcl/sparsity-in-deep-learning)]
- Learning both Weights and Connections for Efficient Neural Networks [[✏️✏️NIPS](https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html)]
- Learning Efficient Convolutional Networks Through Network Slimming [[✏️✏️ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.html)][⭐⭐[github 1.](https://github.com/Eric-mingjie/network-slimming)[github 1.](https://github.com/Eric-mingjie/network-slimming)]
- Learning Filter Basis for Convolutional Neural Network Compression [[✏️ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Learning_Filter_Basis_for_Convolutional_Neural_Network_Compression_ICCV_2019_paper.html)]
- Pruning Filters for Efficient ConvNets [[✏️✏️paper](https://arxiv.org/abs/1608.08710)]
- Pruning from Scratch [[✏️AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/6910)]
- Recent Advances in Efficient Computation of Deep Convolutional Neural Networks [[✏️paper](https://scholar.google.com/scholar?hl=es&as_sdt=0%2C5&q=Recent+Advances+in+Efficient+Computation+of+Deep+Convolutional+Neural+Networks&btnG=#:~:text=ACNP%20Full%20Text-,Recent%20advances%20in%20efficient%20computation%20of%20deep%20convolutional%20neural%20networks,-J%20Cheng%2C)]
- Rethinking the Value of Network Pruning [[✏️✏️paper](https://arxiv.org/abs/1810.05270)][⭐⭐[github](https://github.com/Eric-mingjie/rethinking-network-pruning)]
- The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks [[✏️✏️paper](https://arxiv.org/abs/1803.03635)]
- To prune, or not to prune: exploring the efficacy of pruning for model compression [[✏️✏️paper](https://arxiv.org/abs/1710.01878)]
<br>

### Papers
2021
- A Probabilistic Approach to Neural Network Pruning [[PMLR](http://proceedings.mlr.press/v139/qian21a.html)]
- Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework [[PMLR](http://proceedings.mlr.press/v139/wang21e.html?ref=https://githubhelp.com)]
- ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations [[paper](https://arxiv.org/abs/2102.07156)][⭐⭐[github](https://github.com/transmuteAI/ChipNet)]
- Content-Aware GAN Compression [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Content-Aware_GAN_Compression_CVPR_2021_paper.html)][⭐⭐[github](https://github.com/lychenyoko/content-aware-gan-compression)]
- Convolutional Neural Network Pruning with Structural Redundancy Reduction [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Convolutional_Neural_Network_Pruning_With_Structural_Redundancy_Reduction_CVPR_2021_paper.html)]
- Dynamic Network Surgery for Efficient DNNs [[✏️✏️NIPS](https://proceedings.neurips.cc/paper/2016/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html)][⭐⭐[github](https://github.com/yiwenguo/Dynamic-Network-Surgery)]
- Group Fisher Pruning for Practical Network Compression [[PMLR](http://proceedings.mlr.press/v139/liu21ab.html?ref=https://githubhelp.com)]
- Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Yao_Joint-DetNAS_Upgrade_Your_Detector_With_NAS_Pruning_and_Dynamic_Distillation_CVPR_2021_paper.html)]
- Manifold Regularized Dynamic Network Pruning [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_Manifold_Regularized_Dynamic_Network_Pruning_CVPR_2021_paper.html)]
- Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network [[paper](https://arxiv.org/abs/2103.09377)][⭐⭐[github](https://github.com/chrundle/biprop)]
- Network Pruning That Matters: A Case Study on Retraining Variants [[paper](https://arxiv.org/abs/2105.03193)][⭐⭐[github](https://github.com/lehduong/NPTM)]
- Network Pruning via Performance Maximization [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Network_Pruning_via_Performance_Maximization_CVPR_2021_paper.html)]
- NPAS: A Compiler-aware Framework of Unified Network Pruning andArchitecture Search for Beyond Real-Time Mobile Acceleration [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Li_NPAS_A_Compiler-Aware_Framework_of_Unified_Network_Pruning_and_Architecture_CVPR_2021_paper.html)]
- On the Predictability of Pruning Across Scales [[PMLR](http://proceedings.mlr.press/v139/rosenfeld21a.html)]
- Prune Once for All: Sparse Pre-Trained Language Models [[paper](https://arxiv.org/abs/2111.05754)][⭐⭐[github](https://github.com/intellabs/model-compression-research-package)]
- Pruning Neural Networks at Initialization: Why Are We Missing the Mark? [[✏️paper](https://arxiv.org/abs/2009.08576)]
- Towards Compact CNNs via Collaborative Compression [[CVPR](https://arxiv.org/abs/2105.11228)]


2020

- A Gradient Flow Framework For Analyzing Network Pruning [[paper](https://arxiv.org/abs/2009.11839)][⭐⭐[github](https://github.com/EkdeepSLubana/flowandprune)]
- A Signal Propagation Perspective for Pruning Neural Networks at Initialization [[✏️paper](https://arxiv.org/abs/1906.06307)][⭐⭐[github](https://github.com/namhoonlee/spp-public)]
- Accelerating CNN Training by Pruning Activation Gradients [[ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58595-2_20)]
- Adversarial Neural Pruning with Latent Vulnerability Suppression [[PMLR](http://proceedings.mlr.press/v119/madaan20a.html)][⭐⭐[github](https://github.com/divyam3897/ANP_VS)]
- AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates [[✏️AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/5924)]
- Bayesian Bits: Unifying Quantization and Pruning [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Abstract.html)]
- Channel Pruning via Automatic Structure Search [[✏️paper](https://arxiv.org/abs/2001.08565)][⭐⭐[github](https://github.com/lmbxmu/ABCPruner)]
- Comparing Rewinding and Fine-tuning in Neural Network Pruning [[✏️paper](https://arxiv.org/abs/2003.02389)][⭐⭐[github](https://github.com/lottery-ticket/rewinding-iclr20-public)]
- DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search [[ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58583-9_35)]
- DHP: Differentiable Meta Pruning via HyperNetworks [[✏️ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58598-3_36)][⭐⭐[github](https://github.com/ofsoundof/dhp)]
- Differentiable Joint Pruning and Quantization for Hardware Efficiency [[ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58526-6_16)]
- Directional Pruning of Deep Neural Networks [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/a09e75c5c86a7bf6582d2b4d75aad615-Abstract.html)][⭐⭐[github](https://github.com/donlan2710/gRDA-Optimizer/tree/master/directional_pruning)]
- Discrete Model Compression With Resource Constraint for Deep Neural Networks [[CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Gao_Discrete_Model_Compression_With_Resource_Constraint_for_Deep_Neural_Networks_CVPR_2020_paper.html)]
- DMCP: Differentiable Markov Channel Pruning for Neural Networks [[✏️CVPR](https://arxiv.org/abs/2005.03354)][⭐⭐[github](https://arxiv.org/abs/2005.03354)]
- DropNet: Reducing Neural Network Complexity via Iterative Pruning [[PMLR](http://proceedings.mlr.press/v119/tan20a.html)][⭐⭐[github](https://github.com/tanchongmin/DropNet)]
- DSA: More Efficient Budgeted Pruning via Differentiable Sparsity Allocation [[ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58580-8_35)]
- Dynamic Model Pruning with Feedback [[✏️paper](https://arxiv.org/abs/2006.07253)]
- EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning [[✏️ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58536-5_38)][⭐⭐[github](https://github.com/anonymous47823493/EagleEye)]
- Fast Convex Pruning of Deep Neural Networks [[paper](https://arxiv.org/pdf/1806.06457.pdf)]
- Few Sample Knowledge Distillation for Efficient Network Compression [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.html)]
- Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection [[✏️PMLR](https://proceedings.mlr.press/v119/ye20b.html)][⭐⭐[github](https://github.com/lushleaf/Network-Pruning-Greedy-Forward-Selection)]
- Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Group_Sparsity_The_Hinge_Between_Filter_Pruning_and_Decomposition_for_CVPR_2020_paper.html)][⭐⭐[github](https://github.com/ofsoundof/group_sparsity)]
- HRank: Filter Pruning using High-Rank Feature Map [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Lin_HRank_Filter_Pruning_Using_High-Rank_Feature_Map_CVPR_2020_paper.html)][⭐⭐[github](https://github.com/lmbxmu/HRank)]
- HYDRA: Pruning Adversarially Robust Neural Networks [[✏️NIPS](https://proceedings.neurips.cc/paper/2020/hash/e3a72c791a69f87b05ea7742e04430ed-Abstract.html)]
- Layer-adaptive Sparsity for the Magnitude-based Pruning [[paper](https://arxiv.org/abs/2010.07611)][⭐⭐[github](https://github.com/jaeho-lee/layer-adaptive-sparsity)]
- Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/He_Learning_Filter_Pruning_Criteria_for_Deep_Convolutional_Neural_Networks_Acceleration_CVPR_2020_paper.html)]
- Logarithmic Pruning is All You Need [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/1e9491470749d5b0e361ce4f0b24d037-Abstract.html)]
- Lookahead: A Far-sighted Alternative of Magnitude-based Pruning [[paper](https://arxiv.org/abs/2002.04809)][⭐⭐[github](https://github.com/alinlab/lookahead_pruning)]
- Meta-Learning with Network Pruning [[ECCV](https://arxiv.org/abs/2007.03219)]
- Movement Pruning: Adaptive Sparsity by Fine-Tuning [[✏️NIPS](https://proceedings.neurips.cc/paper/2020/hash/eae15aabaa768ae4a5993a8a4f4fa6e4-Abstract.html)]
- Multi-Dimensional Pruning: A Unified Framework for Model Compression [[CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Guo_Multi-Dimensional_Pruning_A_Unified_Framework_for_Model_Compression_CVPR_2020_paper.html)]
- Neural Network Pruning with Residual-Connections and Limited-Data [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Luo_Neural_Network_Pruning_With_Residual-Connections_and_Limited-Data_CVPR_2020_paper.html)]
- Neural Pruning via Growing Regularization [[paper](https://arxiv.org/abs/2012.09243)][⭐⭐[github](https://github.com/mingsun-tse/regularization-pruning)]
- Neuron Merging: Compensating for Pruned Neurons [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/0678ca2eae02d542cc931e81b74de122-Abstract.html)][⭐⭐[github](https://github.com/friendshipkim/neuron-merging)]
- Neuron-level Structured Pruning using Polarization Regularizer [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/703957b6dd9e3a7980e040bee50ded65-Abstract.html)]
- Operation-Aware Soft Channel Pruning using Differentiable Masks [[✏️PMLR](https://proceedings.mlr.press/v119/kang20a.html)]
- Position-based Scaled Gradient for Model Quantization and Pruning [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/eb1e78328c46506b46a4ac4a1e378b91-Abstract.html)]
- ProxSGD: Training Structured Neural Networks under Regularization and Constraints [[ICLR](https://orbilu.uni.lu/bitstream/10993/45122/1/proxsgd_training_structured_neural_networks_under_regularization_and_constraints.pdf)]
- Pruning Filter in Filter [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/ccb1d45fb76f7c5a0bf619f979c6cf36-Abstract.html)]
- Pruning neural networks without any data by iteratively conserving synaptic flow [[✏️NIPS](https://proceedings.neurips.cc/paper/2020/hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html)]
- Reborn filters: Pruning convolutional neural networks with limited data [[AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/6058)]
- Robust Pruning at Initialization [[paper](https://arxiv.org/abs/2002.08797)]
- Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/eae27d77ca20db309e056e3d2dcd7d69-Abstract.html)]
- SCOP: Scientific Control for Reliable Neural Network PruningNeurIPSFPyTorch [[✏️NIPS](https://proceedings.neurips.cc/paper/2020/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html)]
- Soft Threshold Weight Reparameterization for Learnable Sparsity [[✏️PMLR](https://proceedings.mlr.press/v119/kusupati20a.html)][⭐⭐[github](https://github.com/RAIVNLab/STR)]
- Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/a914ecef9c12ffdb9bede64bb703d877-Abstract.html)]
- Structured Compression by Weight Encryption for Unstructured Pruning and Quantization [[CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Kwon_Structured_Compression_by_Weight_Encryption_for_Unstructured_Pruning_and_Quantization_CVPR_2020_paper.html)]
- The Generalization-Stability Tradeoff In Neural Network Pruning [[NIPS](https://proceedings.neurips.cc/paper/2020/hash/ef2ee09ea9551de88bc11fd7eeea93b0-Abstract.html)]
- Towards Efficient Model Compression via Learned Global Ranking [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Chin_Towards_Efficient_Model_Compression_via_Learned_Global_Ranking_CVPR_2020_paper.html)][⭐⭐[github](https://github.com/enyac-group/LeGR)]


2019
- Accelerate CNN via Recursive Bayesian Pruning [[CVPR](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Accelerate_CNN_via_Recursive_Bayesian_Pruning_ICCV_2019_paper.html)]
- Adversarial Robustness vs Model Compression, or Both? [[✏️ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.html)]
- Approximated Oracle Filter Pruning for Destructive CNN Width Optimization github [[✏️PMLR](http://proceedings.mlr.press/v97/ding19a.html)][⭐⭐[github](https://github.com/DingXiaoH/AOFP)]
- AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters [[✏️NIPS](https://proceedings.neurips.cc/paper/2019/hash/4efc9e02abdab6b6166251918570a307-Abstract.html)]
- Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Centripetal_SGD_for_Pruning_Very_Deep_Convolutional_Networks_With_Complicated_CVPR_2019_paper.html)]
- Collaborative Channel Pruning for Deep Networks [[✏️PMLR](http://proceedings.mlr.press/v97/peng19c.html?ref=https://githubhelp.com)]
- COP: Customized Deep Model Compression via Regularized Correlation-Based Filter-Level Pruning [[paper](https://arxiv.org/abs/1906.10337)][⭐⭐[github](https://github.com/ZJULearning/COP)]
- DARB: A Density-Aware Regular-Block Pruning for Deep Neural Networks [[paper](https://arxiv.org/abs/1911.08020)]
- Data-Independent Neural Pruning via Coresets [[✏️paper](https://arxiv.org/abs/1907.04018)]
- EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis4 [[✏️ICML](https://proceedings.mlr.press/v97/wang19g.html)][⭐⭐[github](https://github.com/alecwangcq/EigenDamage-Pytorch)]
- Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration [[✏️✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/He_Filter_Pruning_via_Geometric_Median_for_Deep_Convolutional_Neural_Networks_CVPR_2019_paper.html)]
- Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks [[✏️NIPS](https://proceedings.neurips.cc/paper/2019/hash/b51a15f382ac914391a58850ab343b00-Abstract.html)]
- Global Sparse Momentum SGD for Pruning Very Deep Neural Networks [[✏️NIPS](https://proceedings.neurips.cc/paper/2019/hash/f34185c4ca5d58e781d4f14173d41e5d-Abstract.html)]
- Importance Estimation for Neural Network Pruning [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Molchanov_Importance_Estimation_for_Neural_Network_Pruning_CVPR_2019_paper.html)]
- MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning [[✏️ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_MetaPruning_Meta_Learning_for_Automatic_Neural_Network_Channel_Pruning_ICCV_2019_paper.html)]
- Model Compression with Adversarial Robustness: A Unified Optimization Framework [[✏️NIPS](https://proceedings.neurips.cc/paper/2019/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html)]
- Network Pruning via Transformable Architecture Search [[✏️NIPS](https://proceedings.neurips.cc/paper/2019/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html)][⭐⭐[github](https://github.com/D-X-Y/NAS-Projects)]
- OICSR: Out-In-Channel Sparsity Regularization for Compact Deep Neural Networks [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_OICSR_Out-In-Channel_Sparsity_Regularization_for_Compact_Deep_Neural_Networks_CVPR_2019_paper.html)]
- On Implicit Filter Level Sparsity in Convolutional Neural Networks [[CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Mehta_On_Implicit_Filter_Level_Sparsity_in_Convolutional_Neural_Networks_CVPR_2019_paper.html)]
- One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers [[✏️NIPS](https://proceedings.neurips.cc/paper/2019/hash/a4613e8d72a61b3b69b32d040f89ad81-Abstract.html)]
- One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation [[paper](https://arxiv.org/abs/1912.00120)]
- Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural Architecture Search [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Partial_Order_Pruning_For_Best_SpeedAccuracy_Trade-Off_in_Neural_Architecture_CVPR_2019_paper.html)]
- Provable Filter Pruning for Efficient Neural Networks [[✏️paper](https://arxiv.org/abs/1911.07412)][⭐⭐[github](https://github.com/lucaslie/provable_pruning)]
- Structured Pruning of Neural Networks with Budget-Aware Regularization [[✏️CVPR](https://arxiv.org/abs/1811.09332)]
- The State of Sparsity in Deep Neural Networks [[✏️paper](https://arxiv.org/pdf/1902.09574.pdf)][⭐⭐[github](https://github.com/bayesgroup/variational-dropout-sparsifies-dnn)]
- Towards Optimal Structured CNN Pruning via Generative Adversarial Learning [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Lin_Towards_Optimal_Structured_CNN_Pruning_via_Generative_Adversarial_Learning_CVPR_2019_paper.html)]
- Variational Convolutional Neural Network Pruning [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Variational_Convolutional_Neural_Network_Pruning_CVPR_2019_paper.html)]



2018

- A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers [[✏️ECCV](https://openaccess.thecvf.com/content_ECCV_2018/html/Tianyun_Zhang_A_Systematic_DNN_ECCV_2018_paper.html)][⭐⭐[github](https://github.com/KaiqiZhang/admm-pruning)]
- Accelerating Convolutional Networks via Global & Dynamic Filter Pruning [[✏️NIPS](https://www.ijcai.org/proceedings/2018/0336.pdf)]
- Amc: Automl for model compression and acceleration on mobile devices [[✏️✏️ECCV](https://openaccess.thecvf.com/content_ECCV_2018/html/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.html)][⭐⭐[github](https://github.com/mit-han-lab/amc)]
- CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization [[✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Tung_CLIP-Q_Deep_Network_CVPR_2018_paper.html)]
- Constraint-Aware Deep Neural Network Compression [[✏️ECCV](https://openaccess.thecvf.com/content_ECCV_2018/html/Changan_Chen_Constraints_Matter_in_ECCV_2018_paper.html)]
- Coreset-Based Neural Network Compression [[✏️ECCV](https://openaccess.thecvf.com/content_ECCV_2018/html/Abhimanyu_Dubey_Coreset-Based_Convolutional_Neural_ECCV_2018_paper.html)]
- Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds [[✏️paper](https://arxiv.org/abs/1804.05345)]
- Data-Driven Sparse Structure Selection for Deep Neural Networks [[✏️ECCV](https://arxiv.org/abs/1707.01213v3)][⭐⭐[github](https://github.com/huangzehao/sparse-structure-selection)]
- Discrimination-aware Channel Pruning for Deep Neural Networks [[✏️NIPS](https://proceedings.neurips.cc/paper/2018/hash/55a7cf9c71f1c9c495413f934dd1a158-Abstract.html)]
- Dynamic Channel Pruning: Feature Boosting and Suppression [[✏️paper](https://arxiv.org/abs/1810.05331)][⭐⭐[github](https://github.com/deep-fry/mayo)]
- Dynamic Sparse Graph for Efficient Deep Learning [[paper](https://arxiv.org/abs/1810.00859)]
- Frequency-Domain Dynamic Pruning for Convolutional Neural Networks [[✏️NIPS](https://proceedings.neurips.cc/paper/2018/hash/a9a6653e48976138166de32772b1bf40-Abstract.html)]
- “Learning-Compression” Algorithms for Neural Net Pruning [[✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.html)]
- Learning Sparse Neural Networks via Sensitivity-Driven Regularization [[✏️NIPS](https://proceedings.neurips.cc/paper/2018/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html)]
- Model Compression and Acceleration for Deep Neural Networks [[✏️IEEE](https://sci-hub.se/10.1109/MSP.2017.2765695)]"
- NISP: Pruning Networks using Neuron Importance Score Propagation [[✏️✏️CVPR](https://arxiv.org/abs/1711.05908)]
- PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning [[✏️✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html)][⭐⭐[github](https://github.com/arunmallya/packnet)]
- Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers [[✏️paper](https://arxiv.org/abs/1802.00124)][⭐⭐[github](https://github.com/bobye/batchnorm_prune)]
- SNIP: Single-shot Network Pruning based on Connection Sensitivity [[✏️✏️paper](https://arxiv.org/abs/1810.02340)][⭐⭐[github](https://github.com/namhoonlee/snip-public)]
- Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks [[✏️✏️paper](https://arxiv.org/abs/1808.06866)][⭐⭐[github](https://github.com/he-y/soft-filter-pruning)]
- Stronger generalization bounds for deep nets via a compression approach [[✏️PMLR](http://proceedings.mlr.press/v80/arora18b/arora18b.pdf)]


2017

- Channel pruning for accelerating very deep neural networks [[✏️✏️ICCV](http://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html)][⭐⭐[github](https://github.com/yihui-he/channel-pruning)]
- Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning [[✏️✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Designing_Energy-Efficient_Convolutional_CVPR_2017_paper.html)]
- Efficient Processing of Deep Neural Networks: A Tutorial and Survey [[✏️✏️IEEE](https://sci-hub.se/10.1109/JPROC.2017.2761740)]
- Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon [[✏️NIPS](https://proceedings.neurips.cc/paper/2017/hash/c5dc3e08849bec07e33ca353de62ea04-Abstract.html)]
- Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee [[✏️NIPS](https://proceedings.neurips.cc/paper/2017/hash/3fab5890d8113d0b5a4178201dc842ad-Abstract.html)]
- Runtime Neural Pruning [[✏️NIPS](https://proceedings.neurips.cc/paper/2017/hash/a51fb975227d6640e4fe47854476d133-Abstract.html)]



2016

- Pruning Convolutional Neural Networks for Resource Efficient Inference [[✏️✏️paper](https://arxiv.org/abs/1611.06440)]


1989

- Optimal Brain Damage (LeCun) [[✏️✏️NIPS](https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf)]
<br>


## Courses, webinars and blogs

Courses
- Stanford 2020, Hardware Accelerators for Machine Learning [[paper](https://cs217.stanford.edu)]

Webinars, video content
- Cornell 2022, ML HW & Systems [[paper](https://www.youtube.com/watch?v=66HCPAcimGY&list=PL0mFAhrXqy9CuopJhAB8GVu_Oy7J0ery6)]
- MIT 2021, Vivienne Sze & Lex Fridman, Efficient Computing for Deep Learning, Robotics, and AI [[paper]( https://www.youtube.com/watch?v=WbLQqPw_n88)]
- Stanford 2017, Son Han, Efficient Methods and Hardware for Deep Learning [[paper](https://www.youtube.com/watch?v=eZdOkDtYMoo)]


Blogs, written content
- Adding Quantization-aware Training and Pruning to the TensorFlow Model Garden [[paper](https://blog.tensorflow.org/2022/06/Adding-Quantization-aware-Training-and-Pruning-to-the-TensorFlow-Model-Garden.html)]
- A friendly introduction to machine learning compilers and optimizers [[paper](https://huyenchip.com/2021/09/07/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html)]
- Faster Deep Learning Training with PyTorch – a 2021 Guide [[paper](https://efficientdl.com/faster-deep-learning-in-pytorch-a-guide/)]
- Optimize machine learning models with Tensorflow [[paper](https://www.tensorflow.org/model_optimization)]





<img height="25" width="100%" src="https://user-images.githubusercontent.com/83510798/171454644-d4b980bc-15ab-4a31-847c-75c36c5bd96b.png">

<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> •
  <a href="https://github.com/nebuly-ai/learning-AI-optimization#contribute">Contribute to the library</a>
</p>










